{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPEN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text=\"What is the capital of India\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\indtal\\OneDrive - Vanderlande\\Desktop\\chatGPT\\Langchain\\gen-ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\indtal\\OneDrive - Vanderlande\\Desktop\\chatGPT\\Langchain\\gen-ai\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm_huggingface=HuggingFaceHub(repo_id=\"google/flan-t5-large\",model_kwargs={\"temperature\":0,\"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_huggingface.predict(\"Can you tell the capital of russia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chess is for people who aren't smart enough to do anything else. It's a game for people who don't have the brains to figure out something better to do with their time. It's a complete waste of time and energy.\n"
     ]
    }
   ],
   "source": [
    "## Prompt Templates\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"sport\", \"behaviour\"], template=\"Tell me about the sport {sport} in a {behaviour} way\")\n",
    "llm=OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run({\"sport\":\"Chess\", \"behaviour\":\"rude and arrogant\"}))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'city': 'kolkata',\n",
       " 'restaurant': \"\\n\\nThe best restaurant in Kolkata is Indian Accent. It offers a variety of dishes from all over India, as well as some international cuisine. The restaurant has been awarded multiple awards and accolades, including the 'Best Restaurant in India' award from the Times Food Guide. It is also one of the few Michelin-starred restaurants in the city.\",\n",
       " 'dish': ' The most famous dish served at Indian Accent is the Lamb Raan, a succulent, slow-cooked lamb dish served with a variety of accompaniments.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "prompt1 = PromptTemplate(input_variables=[\"city\"], template=\"Which is the best retaurant in {city}\")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"restaurant\")\n",
    "\n",
    "prompt2 = PromptTemplate(input_variables=[\"restaurant\"], template=\"Which is the most famous dish in {restaurant}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"dish\")\n",
    "\n",
    "chain = SequentialChain(chains=[chain1,chain2], input_variables=[\"city\"], output_variables=[\"restaurant\",\"dish\"], verbose=True)\n",
    "chain(\"kolkata\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatModels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why don\\'t AI robots go on dates? Because they can\\'t compute \\'chemistry\\'!\"\\n\\n2. \"Why was the computer cold? It left its Windows open!\"\\n\\n3. \"Why don\\'t AI assistants ever get lost? Because they always keep a \\'tab\\' open!\"\\n\\n4. \"Why was the computer broke? It had a bad case of \\'CAPS LOCK\\'!\"\\n\\n5. \"Why did the AI cross the road? Because it was programmed to disrupt traffic patterns!\"\\n\\n6. \"Why don\\'t computers take their coffee with sugar? Because they prefer to have their Java \\'coding\\' black!\"\\n\\n7. \"Why did the AI go to therapy? Because it had too many \\'processing\\' issues!\"\\n\\n8. \"How does an AI say goodbye? \\'It\\'s not you, it\\'s me...and by \\'me\\', I mean my programming\\'!\"\\n\\n9. \"Why is the AI a terrible storyteller? Because it always \\'data dumps\\' the punchline!\"\\n\\n10. \"Why did the AI break up with its girlfriend? Because she had too many \\'bugs\\'!\"')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm=ChatOpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6,model='gpt-4')\n",
    "chatllm([\n",
    "SystemMessage(content=\"Yor are a comedian AI assitant\"),\n",
    "HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(\",\")\n",
    "\n",
    "template=\"Your are a helpful assistant. When the use given any input , you should generate 5 words synonyms in a comma seperated list\"\n",
    "human_template=\"{text}\"\n",
    "chatprompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=chatprompt|chatllm|Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' bright', ' brilliant', ' knowledgeable']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
